{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Ad Auction Dataset\n",
    "## Team: The Puffy Shirts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data set:\n",
    "\n",
    "    30 days of CSV files    \n",
    "    rows are winning bids in a second-price auction\n",
    "    contains information on the target user/platform, the UTC timestamp, the app, the exchange, etc...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges of this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large data set:\n",
    "\n",
    "- 1.42 GB as .zip / 8.19 GB as .csv\n",
    "    \n",
    "- 36.3M rows, 26 columns\n",
    "    \n",
    "Highly unbalanced categorical classification problem:\n",
    "\n",
    "- 1.57M clicks (4.3% of rows)\n",
    "    \n",
    "- 2,109 installs (0.0058% of rows, 0.13% of clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 240 # fix high-dpi display scaling issues\n",
    "\n",
    "sys.path.append(os.getcwd()) # add cwd to path\n",
    "\n",
    "from zip_codes import ZC # zip code database\n",
    "import load_file as lf # file i/o\n",
    "import myplots as mp # my plotting functions\n",
    "zc = ZC('') # initialize zip code class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_27 = '2019-04-27.csv'\n",
    "data_dir = r'C:\\PythonBC\\RootData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_27 = pd.read_csv( os.path.join(data_dir, fname_27))\n",
    "end = time.time()\n",
    "print(f'loading {fname_27} with default pd.read_csv() settings takes'\n",
    "      + f' {end-start:3.2f} seconds and {lf.mem_usage(df_27)} of RAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some columns can be immediately dropped:\n",
    "- auction_id (meaningless string)\n",
    "- platform_os (all 'Android' or '-1')\n",
    "- app_bundle (all the same)\n",
    "- year (UTC time)\n",
    "- month (UTC time)\n",
    "- day (UTC time)\n",
    "- day_of_week (UTC time, incorrectly calculated)\n",
    "- hour (UTC time)\n",
    "- creative_size (redundant with creative_type)\n",
    "\n",
    "most columns are categorical with only a few unique values:\n",
    "- inventory_source (only 4 possible values)\n",
    "- category\n",
    "- platform_bandwidth\n",
    "- platform_carrier\n",
    "- platform_device_make (only 5 possible values)\n",
    "- platform_device_model\n",
    "- platform_device_screen_size\n",
    "- geo_zip\n",
    "\n",
    "some are boolean:\n",
    "- inventory_interstitial\n",
    "- rewarded\n",
    "- clicks\n",
    "- installs\n",
    "\n",
    "some columns need additional work:\n",
    "- geo_zip ('43212.0' >>> '43212')\n",
    "- category & segments are unsorted string lists\n",
    "\n",
    "some columns have multiple nan values:\n",
    "- platform_carrier\n",
    "- platform_device_make\n",
    "- platform_device_model\n",
    "- platform_device_screen_size\n",
    "\n",
    "The function lf.load_data() drops the useless columns and cleans & downcasts the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#df_27 = lf.load_wrapper(fname=fname_27, data_dir=data_dir)\n",
    "df_27 = lf.load_data(fname=fname_27, data_dir=data_dir)\n",
    "end = time.time()\n",
    "print(f'loading {fname_27} with downcasting takes {end-start:3.2f}'\n",
    "      + f' seconds and {lf.mem_usage(df_27)} of RAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these tricks we reduce the RAM usage by a factor of **9.1**, but increase the load time by 43%. With this RAM reduction we can comfortably load the entire dataset into memory (~ 850 MB).\n",
    "\n",
    "We can quickly save the processed dataframe to disk using pd.to_parquet():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_save = time.time()\n",
    "lf.temp_save(df_27, fname=os.path.join(data_dir, fname_27.split('.')[0]+'.gzip'))\n",
    "end_save = time.time()\n",
    "df_27 = lf.temp_load( os.path.join(data_dir, fname_27.split('.')[0]+'.gzip') )\n",
    "end_load = time.time()\n",
    "\n",
    "print(f'saving to .gzip takes {end_save-start_save:3.2f}'\n",
    "      + f' seconds and loading from .gzip takes {end_load-end_save:3.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because parquet is so fast, it makes sense to pre-process all the data and save it to disk for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the geo_zip column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a zip code database (https://www.unitedstateszipcodes.org/zip-code-database/) to get local information for each bid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myzip = ['43210']\n",
    "print(f'zip code {myzip[0]}:')\n",
    "print( f'   state: {zc.zip_to_state_2(myzip)[0]}' )\n",
    "print( f'   county: {zc.zip_to_county_2(myzip)[0]}' )\n",
    "print( f'   timezone: {zc.zip_to_tz_2(myzip)[0]}' )\n",
    "print( f'   coordinates: {zc.zip_to_lat_2(myzip)[0], zc.zip_to_lon_2(myzip)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the local time zone, we can shift bid_timestamp_utc to local time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'UTC timestamp = {df_27.bid_timestamp_utc.iloc[0]}')\n",
    "print(f'local timestamp = {df_27.bid_timestamp_utc.iloc[0].tz_convert(\"America/New_York\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is slow, so run it on each .csv file separately and combine the results at the end.\n",
    "\n",
    "With the local timestamp, we can extract quantities like hour, day and day_of_week.\n",
    "\n",
    "The functions reshape_files() and local_hour_creator() do the aforementioned preprocessing and save each column as a .gzip file. This step takes about 30-40 minutes on a laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization: maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data in memory, we can make visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choropleths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "state_geo = 'us-states.json' # local copy of json file\n",
    "\n",
    "# load relevant data from disk\n",
    "data_dir = r'C:\\PythonBC\\RootData'\n",
    "df_clicks = lf.temp_load( os.path.join(data_dir, 'clicks.gzip')  )\n",
    "df_state = lf.temp_load( os.path.join(data_dir, 'state.gzip')  )\n",
    "df_installs = lf.temp_load( os.path.join(data_dir, 'installs.gzip')  )\n",
    "frames = [df_state, df_clicks, df_installs]\n",
    "df = pd.concat(frames, axis=1)\n",
    "\n",
    "# number of clicks per state\n",
    "dfstates = df.groupby('state').sum()['clicks'].to_frame()\n",
    "dfstates.reset_index(level=0, inplace=True)\n",
    "\n",
    "# number of bids per state\n",
    "dfstates2 = df.groupby('state').count()['clicks'].to_frame()\n",
    "dfstates2.reset_index(level=0, inplace=True)\n",
    "dfstates2.rename(index=str, columns={\"state\": \"state\", \"clicks\": \"bids\"})\n",
    "\n",
    "# number of installs per state\n",
    "dfstates3 = df.groupby('state').sum()['installs'].to_frame()\n",
    "dfstates3.reset_index(level=0, inplace=True)\n",
    "dfstates3.rename(index=str, columns={\"state\": \"state\", \"clicks\": \"installs\"})\n",
    "\n",
    "# build new dataframe\n",
    "bids = dfstates2.clicks.values\n",
    "clicks = dfstates.clicks.values\n",
    "installs = np.asarray(dfstates3.installs)\n",
    "state = dfstates.state.values\n",
    "clickrate = 100*np.divide(clicks, bids)\n",
    "installrate = 100*np.divide(installs, bids)\n",
    "frames = {\"state\": state, \"bids\": bids, \"clicks\": clicks, \"installs\": installs, \"clickrate\":clickrate, \"installrate\": installrate}\n",
    "df_rate = pd.DataFrame(data=frames)\n",
    "df_rate_nonzero = df_rate[df_rate.clickrate > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickrate_m = folium.Map(location=[39.50, -98.35], zoom_start=4) # lower 48\n",
    "folium.Choropleth(\n",
    "    geo_data=state_geo,\n",
    "    name='choropleth',\n",
    "    data=df_rate_nonzero,\n",
    "    columns=['state', 'clickrate'],\n",
    "    key_on='feature.id',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='click rate (%)'\n",
    ").add_to(clickrate_m)\n",
    "folium.LayerControl().add_to(clickrate_m)\n",
    "clickrate_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "installrate_m = folium.Map(location=[39.50, -98.35], zoom_start=4) # lower 48\n",
    "folium.Choropleth(\n",
    "    geo_data=state_geo,\n",
    "    name='choropleth',\n",
    "    data=df_rate_nonzero,\n",
    "    columns=['state', 'installrate'],\n",
    "    key_on='feature.id',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='install rate (%)'\n",
    ").add_to(installrate_m)\n",
    "folium.LayerControl().add_to(installrate_m)\n",
    "installrate_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "zipc = lf.temp_load( os.path.join(data_dir, 'geo_zip.gzip'))\n",
    "click = lf.temp_load( os.path.join(data_dir, 'clicks.gzip'))\n",
    "df = pd.concat([zipc,click],axis=1)\n",
    "\n",
    "dftest = df.query('clicks == True')\n",
    "dftest = pd.DataFrame(dftest.geo_zip.value_counts())\n",
    "df = pd.DataFrame(df.geo_zip.value_counts())\n",
    "df['latitude'] = zc.zip_to_lat_2(df.index)\n",
    "df['longitude'] = zc.zip_to_lon_2(df.index)\n",
    "\n",
    "dfratio = dftest/df\n",
    "dfratio['latitude'] = zc.zip_to_lat_2(dfratio.index)\n",
    "dfratio['longitude'] = zc.zip_to_lon_2(dfratio.index)\n",
    "\n",
    "df=df.dropna()\n",
    "dfratio=dfratio.dropna()\n",
    "\n",
    "df = df[['latitude','longitude','geo_zip']]\n",
    "df_copy = df.copy()\n",
    "\n",
    "dfratio = dfratio[['latitude','longitude','geo_zip']]\n",
    "dfratio_copy = dfratio.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Heatmap of ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[38.5, -100],zoom_start=4)\n",
    "HeatMap(data=df_copy[['latitude', 'longitude', 'geo_zip']].groupby(['latitude', 'longitude']).\\\n",
    "        sum().reset_index().values.tolist(), radius=6, max_zoom=13).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of (clicks/bids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mratio = folium.Map(location=[38.5, -100],zoom_start=4)\n",
    "HeatMap(data=dfratio_copy[['latitude', 'longitude', 'geo_zip']].groupby(['latitude', 'longitude']).\\\n",
    "        sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(mratio)\n",
    "mratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded into dataframe and some columns are dropped. Masks are generated to split the data between the first three weeks and the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in tqdm(glob.glob(os.path.join(data_directory, '*.gzip'))):\n",
    "\tdf = pd.concat([df,lf.temp_load(fname=f)], axis=1)\n",
    "\t\n",
    "df = df.drop(['app_bundle','bid_timestamp_utc', 'tz', 'spend', 'installs'], axis=1)\n",
    "df['inventory_interstitial'] = df['inventory_interstitial'].astype(int)\n",
    "df['rewarded'] = df['rewarded'].astype(int)\n",
    "df['clicks'] = df['clicks'].astype(int)\n",
    "\n",
    "start_date = pd.Timestamp('2019-04-23 00:00')\n",
    "end_date = pd.Timestamp('2019-04-30 00:00')\n",
    "mask_lastweek = (df['bid_timestamp_local'] > start_date) & (df['bid_timestamp_local'] <= end_date)\n",
    "\n",
    "start_date = pd.Timestamp('2019-04-01 00:00')\n",
    "end_date = pd.Timestamp('2019-04-23 00:00')\n",
    "mask_threewks = (df['bid_timestamp_local'] > start_date) & (df['bid_timestamp_local'] <= end_date)\n",
    "\n",
    "df = df.drop(['bid_timestamp_local'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the data provide in this data set is categorical in nature. This data needs to be turned in a numerical values for models to train on it.\n",
    "\n",
    "There are several methods to accomplish this:\n",
    "\n",
    "- One Hot Encoding\n",
    "- Hashing\n",
    "- Leave One Out\n",
    "- Ordinal\n",
    "- Binary\n",
    "- ... and many more\n",
    "\n",
    "The scikit-learn-contrib package [category_encoders](https://contrib.scikit-learn.org/categorical-encoding/) provides convenient transformers for many common methods.\n",
    "\n",
    "The encoding method used below is LeaveOneOut, which is good for high cardinality categorical data.\n",
    "\n",
    "From the [documentation](https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html):\n",
    "\n",
    "`For the case of categorical target: features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.clicks.values\n",
    "X = df.drop(['clicks'], axis=1)\n",
    "loo = ce.LeaveOneOutEncoder()\n",
    "X = loo.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to also be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, the data is split into the first three weeks and the last week.  The model will be trained and validated on the first three weeks, and then will be used to predict 'clicks' on the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_threeweek = X[mask_threewks]\n",
    "y_threeweek = y[mask_threewks]\n",
    "X_lastweek = X[mask_lastweek]\n",
    "y_lastweek = y[mask_lastweek]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_threeweek, y_threeweek, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we implement is logistic regression with stochastic gradient descent (SGD).  This was done using SGDClassifier from scikit-learn.  Regularization was implemented by Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.001,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical encoding\n",
    "\n",
    "how you balance the data \n",
    "\n",
    "metrics for evaluating model fit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "trained on 1st 3 weeks:\n",
    "under sampled:\n",
    "![title](img\\RandomUnderSampler_firstweek.png)\n",
    "\n",
    "over sampled:\n",
    "![title](img\\RandomOverSampler_firstweek.png)\n",
    "\n",
    "same model applied to the last week:\n",
    "\n",
    "under sampled:\n",
    "![title](img\\RandomUnderSampler_lastweek.png)\n",
    "\n",
    "over sampled:\n",
    "![title](img\\RandomOverSampler_lastweek.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
